#+TITLE: README 

Document the Steps that were needed to carry out the audEERING coding Task


* Short Version of the Steps

Allgemein: Cookiecutter Data Science Template

Step 1 - Building the dataset

Als Gradle Task implmentiert

Step 2 -  Build Classifier

Benutzt momentan lediglich Functionals und keine llds

Step 3 - Generate Html-Report

Eine Variante ist implemtiert, s. src/models/make_report.py. 
Diese nimmt die papermill bibliothek her und lässt 'report_nb.ipynb' parametrisiert (model name) laufen. Der
Output landet dann in einem intermediate notebook. Dieses wird dann mit nbconvert konvertiert (tmp.html)

Step 4 - Docker Deployment

Unfinished. Bisher ist lediglich das image gebaut. 

* Step 1 - Building the dataset

Dataset creation is done via gradle task, so for recap, a working gradle installation is
required. The database is expected to have been extracted into the `data/raw subdirectory` sich that
the path to the wav-files is `data/raw/wav/` 
The task itself can be invoked via

#+BEGIN_SRC shell
./gradlew makeDataSet 
#+END_SRC

- wav2features.sh: some env variables are hardcoded.
- soll man den Datensatz end2end bauen, also inklusive wget? Hätte den Charme, daß man diesen gleich
  gemeinsam mit einem git clone ins image verbauen kann?
- dann könnte ein daraus abgleiteter Container einen branch pullen und Classifier bauen und report
  gen mitmachen??
- Directory Input

* Step 2 - Build Classifier

- Nur die Functionals wurden Extrahiert, also nicht mehrere Analyseframes per Äusserung

#+BEGIN_SRC shell
./gradlew trainModel 
#+END_SRC

- train_model.py ist ziemlich ad hoc. Idealerweise will der irgendwann mit sklearn BaseEstimator kompatibel werden.

* Step 3 - Generate Report 

Run the model for Report Generation

#+BEGIN_SRC shell
./gradlew makeReportGMMBasic
./gradlew makeReportsvmBasic
#+END_SRC

- Notice: bug in papermill + ipykernel: Läuft trotzdem

- Alternative sacredboard -mu  mongodb://root:example@localhost:27017 mnist
Mongo admin interface; localhost:8081
Sacred: https://pypi.org/project/sacred/
 sacredboard -mu  mongodb://root:example@localhost:27017 torch-bilstm-jean-christophe
 http://localhost:8081/
Show decorators:
 /D/myfiles/2019/Sacred-MNIST/train_convnet.py

- mir fehlt noch die richtige Idee, wie man die Dockerisierung aufsetzt:
reine 


* Deployment Docker

# As a deliverable, please provide access to a Git repository that contains all code and documentation
# to perform the steps above, and provide the report as an assets or webpage. 
# Ideally, everything can be run in a clean sandbox environment
# such as a stock Ubuntu VM or Docker container. It’s perfectly acceptable to
# keep this private, as long as you share it with us.


docker build -t training_image .
Successfully built a58fc48440db
Successfully tagged training_image:latest


docker rm train_model
docker run --name=train_model python-app 




* Informal Final Observations

- Often 




* Misc

https://towardsdatascience.com/beginners-guide-to-data-science-python-docker-3181fd321a5c
https://nbconvert.readthedocs.io/en/latest/execute_api.html
https://elitedatascience.com/imbalanced-classes

https://github.com/trigeorgis/ComParE2017
https://github.com/sercharpak/Emo-DB

Eclipse
https://oremacs.com/2017/03/28/emacs-cpp-ide/
https://www.eclipse.org/forums/index.php/t/1084520/

https://github.com/takluyver/nbparameterise/tree/master/examples

GMM UBM for speaker verification with code
https://wsstriving.github.io/2016/04/28/Code-Based-GMM-UBM-Tutorial/

In the conventional GMM-UBM framework the universal background model (UBM) is a Gaussian mixture
model (GMM) that is trained on a pool of data (known as the background or development data) from a
large number of speakers. The speaker-specific models are then adapted from the UBM using the
maximum a posteriori (MAP) estimation. During the evaluation phase, each test segment is scored
either against all enrolled speaker models to determine who is speaking (speaker identification), or
against the background model and a given speaker model to accept/reject an identity claim (speaker
verification). Functions or modules that will be used for GMM-UBM approach in MSR Identity Toolkit
are depicted in figure 2. 


* Smile Tools 


Hi Christian,

ein kleiner Nachtrag noch zu der Aufgabe, bzw. eine Add-on Aufgabe:

Schau dir bitte auch schonmal den openSMILE source code (C++) etwas an, ausgehend vom main binary
progsrc/smilextract/SMILExtract.cpp, und einzelne Komponenten, wie z.B. src/lldcore/energy.cpp um
dich ein bisschen einzulesen und dann ggf. am Dienstag adhoc ein paar kleine Fragen zum C++ zu
beantworten, insb. z.B. das Energy Feature oder ähnliches zu modifizieren.

VG,
Florian

#+BEGIN_SRC sh
-C /home/christian/bin/opensmile-2.3.0/config/IS13_ComParE.conf  -I /media/win-d/myfiles/2019/emodb-classifier/data/raw/wav/03a01Fa.wav -csvoutput /tmp/results.csv  -appendcsv 1
-C /home/christian/bin/opensmile-2.3.0/config/IS13_ComParE.conf  -I /media/win-d/myfiles/2019/emodb-classifier/data/raw/wav/03a01Fa.wav -csvoutput /tmp/results.csv  -appendcsv 1
SMILExtract -C myconfig/demo1.conf -I ./example-audio/opensmile.wav -O myenergy.csv
#+END_SRC

** Command Line Options

SMILExtract -L lists all components
SMILExtract -H

The bare energy config leider nicht im Pfad
SMILExtract -C config/demo/demo1_energy.conf -I wav_samples/speech01.wav -O speech01.energy.csv

** Voice Activity

 +++ 'cVadV1' +++
   A voice activity detector based on Line-Spectral-Frequencies, Mel spectra and energy + smoothing. This component requires input of the following type in the following order: MelSpec;lsf;energy. See vadV1.hpp for an example config!

** Initialisieren eigener Config (aus dem Buch)

SMILExtract -cfgFileTemplate -cfgFileDescriptions  -configDflt cWaveSource,cFramer,cEnergy,cCsvSink -l 1 2> myconfig/demo1.conf




** Tracing : 

Hier steigt er aus
ConfigType::findField


** Terminologie


*** ‘field’, 

*** ‘element’, 

*** ‘frame’, 

*** ‘window’.

If we view the numeric contents of the data memory level as a 2D <nFields x nTimestemps>
matrix, **frames** correspond to the columns of this matrix, and **windows** or **contours** correspond
the rows of this matrix

- Warum wurde nicht der "The INTERSPEECH 2009 Emotion Challenge feature set" für den Teaser
  hergenommen?
 
** Debug Example - energy2 - Meiner expXXX2

* Docker


https://hackernoon.com/efficient-development-with-docker-and-docker-compose-e354b4d24831

https://runnable.com/docker/python/docker-compose-with-flask-apps


Zusammenhängen von Dockerfiles und docker-compose: 
https://medium.com/bitcraft/docker-composing-a-python-3-flask-app-line-by-line-93b721105777


docker run --name emo-classifier -v ..:/data 

-d detach
docker container stop emo-classifier
$ docker container rm emo-classifier
$ docker volume rm memo-classifier

#  nginx:latest
